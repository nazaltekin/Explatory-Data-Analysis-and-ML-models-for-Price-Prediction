# -*- coding: utf-8 -*-
"""Used Cars Price Prediction Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FwuqLnieIocnfj_nYYRTP3UZ9eIODVqC

# **Used Cars Price Prediction**

### **Data Dictionary**

**S.No.** : Serial Number

**Name** : Name of the car which includes Brand name and Model name

**Location** : The location in which the car is being sold or is available for purchase (Cities)

**Year** : Manufacturing year of the car

**Kilometers_driven** : The total kilometers driven in the car by the previous owner(s) in KM

**Fuel_Type** : The type of fuel used by the car (Petrol, Diesel, Electric, CNG, LPG)

**Transmission** : The type of transmission used by the car (Automatic / Manual)

**Owner** : Type of ownership

**Mileage** : The standard mileage offered by the car company in kmpl or km/kg

**Engine** : The displacement volume of the engine in CC

**Power** : The maximum power of the engine in bhp

**Seats** : The number of seats in the car

**New_Price** : The price of a new car of the same model in INR 100,000

**Price** : The price of the used car in INR 100,000 (**Target Variable**)

### **Loading libraries**
"""

# import libraries for data manipulation
import numpy as np
import pandas as pd

# import libraries for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# To ignore warnings
import warnings
warnings.filterwarnings('ignore')

# Import libraries for building models
# Import Statsmodels
import statsmodels.api as sm
from statsmodels.formula.api import ols
import statsmodels.api as sm

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor #,BaggingRegressor

# # To encode categorical variables
# from sklearn.preprocessing import LabelEncoder

# For tuning the model
from sklearn.model_selection import GridSearchCV

# To check model performance
from sklearn import metrics
# from sklearn.metrics import make_scorer,mean_squared_error, r2_score, mean_absolute_error

"""### **Load the data**"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Great Learning x MIT/Machine Learning/Case Studies/Project: Used Car Price Prediction/Used Cars Data.csv')

"""## Data Overview

- Observations
- Sanity checks
"""

df.head()

df.info()

# Checking for duplicate values
df.duplicated().sum()

# Dropping duplicates
df.drop_duplicates(inplace=True)

# Checking for null values
df.isnull().sum()

"""Observation:
* There are null items in columns : Mileage, Engine, Power, Sears, New Price and Price

## **Exploratory Data Analysis**
"""

df.describe(include='all').T

"""Observations:

* numerical variables: Kilometer_Driven, Mileage, Engine, Power, New_price and Price
  * Milege is between 0 and 33.54, it can't be 0.
  * New price seem to have outliers on max side.
  * Price has a mean much lower than the median, this means outliers.
  * Km driven has a big std deviation, max value appears very large.


* categorical variables= S.No,  Name, Location, Year, Fuel_Type, , Transmission, Owner_Type, Seats
  * There are2041 unique names, meanining 2041 different car models
  * Mumbai has the most used cars.
  * Year is betweeen 1996 and 2019. This seem reasonable.
  * 5 different fuel categories
  * 4 different owner types

Unique values per columns: 2041 Name, 11 Location, 5 Fuel_Type, 2 Transmission, 4 Owner_Types
"""

df_copy = df.copy()

#S.No. column has a unique number for each row, so it doesn't contribute, it can be removed.
df.drop('S.No.', axis=1, inplace=True)

# Checking for extremely large max values compared to remaining data: Km driven
df.sort_values('Kilometers_Driven', ascending=False).head(10)

# Dropping row 2328, because it has a very large value that does not align with rest of the data
df.drop(index=2328,inplace= True)

# Checking for extremely large max values compared to remaining data: Engine
df.sort_values('Engine', ascending=False).head()

# Checking for extremely large max values compared to remaining data: Power
df.sort_values('Power', ascending=False).head(5)

# Checking for extremely large max values compared to remaining data: New_price
df.sort_values('New_price', ascending=False).head()

# Checking for extremely large max values compared to remaining data: Price
df.sort_values('Price', ascending=False).head()

# Removing 0 values for milage
df.sort_values('Mileage', ascending=True).head(10)

"""Observations:
* Km had an outliers, but other checked columns don't
* Mileage had many rows with 0, this will be treated after the univariate analysis to decide how to treat them.

## **Univariate Analysis**
"""

num_vars= ['Kilometers_Driven', 'Mileage', 'Engine', 'Power', 'New_price', 'Price']
cat_vars=['Location', 'Year', 'Fuel_Type', 'Transmission', 'Owner_Type', 'Seats']

plt.figure(figsize=(10,5))
for i in range(len(num_vars)):
  plt.subplot(2,3,i+1)
  sns.histplot(df[num_vars[i]], kde=True) # Changed to histplot for distribution check
  plt.title(num_vars[i])
plt.tight_layout()
plt.show()

"""* Most cars are sold before they reach to 100k km. Kilometers_Driven is highly right-skewed. Log transformation of the feature can be used to reduce/remove the skewness
* Mileage appear to have a normal distribution
* Km driven, engine volume, power, new price and price appear to be right skewed.  There may be a correlation in between.
"""

#Log transformation of Kilometers_driven
sns.displot(np.log(df['Kilometers_Driven']))
plt.show()

"""The log plot is more maningful, kind of left skewed, but it is more clear that the most sold cars are between 10-12 (log based)"""

# Adding a transformed kilometers_driven feature in data
df['kilometers_driven_log']= np.log(df["Kilometers_Driven"])

# Plotting box plots for Price (target var)
plt.figure(figsize=(10,5))
sns.displot(np.log(df['Price']))
plt.show()

"""* The log transformation of the Price variable appears more useful too. It has a normak distribution. So this can also be added to the data frame."""

df["Price_log"] = np.log(df["Price"])

df['Name'].nunique()

"""Name of the car doesn't appear to be a distinctive feature"""

plt.figure(figsize=(15,10))

for i in range(len(cat_vars)):
  plt.subplot(3,3,i+1)
  sns.countplot(x=df[cat_vars[i]])
  plt.xticks(rotation=45, ha='right')
  plt.title(cat_vars[i])
plt.tight_layout()
plt.show()

"""* Most cars are sold in Mumbai
* Most cars are sold between 2011-2019, peaking at 2016
* Most cars are either Diesel or Petrol
* Manual cars are 5k, where automatic cars are 2k
* First owners tend to sell more, and it reduces as the ownership transfer increases
* Most cars are 5 seats

## **Bivariate Analysis**
"""

# Scatter plot for the log transformed values
plt.scatter(df['kilometers_driven_log'],df['Price_log'])
plt.xlabel('kilometers_driven_log')
plt.ylabel('Price_log')
plt.title('Price_log vs Km_driven_log')

# Scatter plot for correlation between other continuos variables and the target
plt.scatter(df['Power'],df['Price_log'])
plt.xlabel('Power')
plt.ylabel('Price_log')
plt.title('Price_log vs Power')

# Scatter plot for correlation between other continuos variables and the target
plt.scatter(df['Engine'],df['Price_log'])
plt.xlabel('Power')
plt.ylabel('Price_log')
plt.title('Price_log vs Power')

# Scatter plot for correlation between other continuos variables and the target
plt.scatter(df['New_price'],df['Price'])
plt.xlabel('Power')
plt.ylabel('Price_log')
plt.title('Price_log vs Power')

"""* The km_driven and price appear to have weak correlation.
* As power, engine volume and new price increase, price increases too.
"""

# Dropping original data for the log transformed values
df_log= df.drop(['Kilometers_Driven','Price'], axis=1)

df_log.head()

# Dropping original data for the log transformed values
plt.figure(figsize=(15,7))
sns.heatmap(df_log.corr(numeric_only = True), annot=True, cmap='coolwarm', vmin = -1, vmax = 1)
plt.show()

"""* Price have positive correlation with new_price, power and engine as discovered earlier.
* New_price has also strong correlation with power with 88%, highest on the table
* Seats does not have a meaningful correlation with other variables.
* Mileage ismoderately negatively correlated with engine and power.
* Year also have some moderate correlation, but not significant.

"""

# This function takes the categorical column as the input and returns the boxplots for the variable.
def boxplot(z):
  plt.figure(figsize=(12,5)) # size of the plot
  sns.boxplot(x=z, y= df['Price']) # boxplot, x as the categorical variable and y as the target
  plt.show()
  plt.figure(figsize=(12,5))
  sns.boxplot(x=z, y= df['Price'],showfliers=False) # turning off the outliers
  plt.show()

boxplot(df['Location'])

boxplot(df['Year'])

boxplot(df['Fuel_Type'])

boxplot(df['Transmission'])

boxplot(df['Owner_Type'])

boxplot(df['Seats'])

"""* Location doesn't tell much, except for Coimbatore, Bangaloer the upper quantile is much larger price than others. Here it is expected the mean price to be higher than the median.
* In general as the year is more recent, median price is higher. In 2009, there is a higher range than the trend.
* Diesel engines appear to have higher price in general. Electric cars have much higher median price than the other fuel types, howerver the spread is low. There may not be sufficient data for this type of fuel.
* Automatic price cars can have much larger price than manual.
* As the owner number increases, price decreases, as expected.
* Two seat cars cost more than others. 4-seats hava large spread.

### **Feature Engineering**
"""

# Name variable has too many unique values. But it includes the brand name, which may be important.
# Extract brand name in a new column, and model to another

df['Brand']= df['Name'].apply(lambda x:x.split(' ')[0].lower())
df['Model']= df['Name'].apply(lambda x:x.split(' ')[1].lower())

df.head()

print('brand count:',df['Brand'].nunique())
df['Brand'].value_counts()

df['Model'].groupby(df['Brand']).value_counts()

"""There are 32 different brands, and brands have several models.

### **Missing value treatment**
"""

df.isnull().sum()

"""* Seats, Engine and Power can be filled using the brand and model of the car
* Mileage canbe filled with the median or other methods
* New price can also be filled using name
* Price is our target valuable, so these rows can be dropped
"""

# checking missing values for column 'Seats'
df[df['Seats'].isnull()]

df.groupby(['Brand','Model'])['Seats'].median()

# Creating a copy of the df to change it
df_copy= df.copy()

"""**Missing values for Seats**"""

# Applying group median based on the unique brand and model combination
df_copy['Seats'] = df_copy.groupby(['Brand', 'Model'])['Seats'].transform(
    lambda x: x.fillna(x.median())
)

df_copy[df_copy['Seats'].isnull()]

# There are still 3 missing values for seats, these can be replaced with the general median in the data
overall_median_seats = df_copy['Seats'].median()
df_copy['Seats'] = df_copy['Seats'].fillna(overall_median_seats)

df_copy[df_copy['Seats'].isnull()]

"""**Missing values for Mileage**"""

df_copy[df_copy['Mileage'].isnull()]

# Applying group median based on the unique brand and model combination
df_copy['Mileage'] = df_copy['Mileage'].transform(
    lambda x: x.fillna(x.median())
)

df_copy[df_copy['Mileage'].isnull()]

"""**Missing values for Engine**"""

# Applying group median based on the unique brand and model combination
df_copy['Engine'] = df_copy.groupby(['Brand', 'Model'])['Engine'].transform(
    lambda x: x.fillna(x.median())
)

df_copy[df_copy['Engine'].isnull()]

"""**Missing values for Power**"""

# Applying group median based on the unique brand and model combination
df_copy['Power'] = df_copy.groupby(['Brand', 'Model'])['Power'].transform(
    lambda x: x.fillna(x.median())
)

# There are still 12 missing values for power, these can be replaced with the general median in the data
overall_median_power = df_copy['Power'].median()
df_copy['Power'] = df_copy['Power'].fillna(overall_median_power)

"""**Missing values for New_price and Price**"""

# Applying group median based on the unique brand and model combination
df_copy['New_price'] = df_copy.groupby(['Brand', 'Model'])['New_price'].transform(
    lambda x: x.fillna(x.median())
)

# There are still 1244 missing values for power, these can be replaced with the general median in the data
overall_new_price_power = df_copy['New_price'].median()
df_copy['New_price'] = df_copy['New_price'].fillna(overall_new_price_power)

# Droping the rows where Price is missing
df_copy.dropna(subset=['Price'], inplace=True)

df_copy.isnull().sum()

"""## **Important Insights from EDA and Data Preprocessing**
* As power, engine volume and new price increase, price increases too.
* Seats does not have a meaningful correlation with other variables.
* Mileage is moderately negatively correlated with engine and power.

* Diesel engines appear to have higher price in general.
* Electric cars have much higher median price than the other fuel types, howerver the spread is low. There may not be sufficient data for this type of fuel.
* Automatic price cars can have much larger price than manual.
* As the owner number increases, price decreases, as expected.
* Two seat cars cost more than others. 4-seats hava large spread.

## **Building Various Models**

### **Split the Data**
"""

df_copy.head(5)

# Seperating the indepdent variables (X) and the dependent variable (y).
# Dropping some variable because:
  # 'Name': not unique,
  # 'Price','Price_log': target variables
  # 'Kilometers_Driven': log transformation will be used
X=df_copy.drop(['Name','Price','Price_log','Kilometers_Driven'],axis=1)
y=df_copy[['Price','Price_log']]

# Encode the categorical variables in X using pd.dummies.
X = pd.get_dummies(X, drop_first = True)

# Split the data into train and test using train_test_split.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(X_train.shape, X_test.shape)

import numpy as np
from sklearn import metrics

# A function to evaluate the r2 scores and RMSE on train and tes data

def evaluate_model(model, flag = True):
  score_list=[] # An empty list to store train and test results

  pred_train = model.predict(X_train)
  pred_train_ =np.exp(pred_train)

  pred_test = model.predict(X_test)
  pred_test_ =np.exp(pred_test)

  train_r2 = metrics.r2_score(y_train['Price'], pred_train_)
  test_r2 = metrics.r2_score(y_test['Price'], pred_test_)

  train_rmse = np.sqrt(metrics.mean_squared_error(y_train['Price'], pred_train_))
  test_rmse = np.sqrt(metrics.mean_squared_error(y_test['Price'], pred_test_))

  score_list.extend((train_r2, test_r2, train_rmse, test_rmse)) # Adding all scores in the list

  # If the flag is set to True then only the following print statements will be dispayed, the default value is True
  if flag == True:

      print("R-sqaure on training set : ", metrics.r2_score(y_train['Price'], pred_train_))

      print("R-square on test set : ", metrics.r2_score(y_test['Price'], pred_test_))

      print("RMSE on training set : ", np.sqrt(metrics.mean_squared_error(y_train['Price'], pred_train_)))

      print("RMSE on test set : ", np.sqrt(metrics.mean_squared_error(y_test['Price'], pred_test_)))

  return score_list

"""For Regression Problems, some of the algorithms used are :<br>

**1) Linear Regression** <br>
**2) Ridge / Lasso Regression** <br>
**3) Decision Trees** <br>
**4) Random Forest** <br>

### **Linear Regression**
"""

# Creating linear regression model using sklearn
lr_model = LinearRegression()

# Fit linear regression model
lr_model.fit(X_train, y_train['Price_log'])

#Scores
LR_score = evaluate_model(lr_model)

"""* The model explains about 94.75% of the variance in the car prices within the training data. This is a very strong fit.And the model explains about 89.57% of the variance in car prices on the unseen test data. This is still a strong score, suggesting, however it is showing possibly overfitting.
* Similarly per RMSE there may be overfitting, the model's prediction is off by 2.52 units in the training set and 3.72 units in the testing set.

**Important variables of Linear Regression**
"""

# Creating anoyher linear regression model with statsmodel
# Setting data type to float to ensure correct numerical type before being passed to the regression model
X_train1 = X_train.astype(float)
y_train1 = y_train.astype(float)

# statasmodel doesn't add an intercept, Adding the constant for the feature matrix
x_train1 = sm.add_constant(X_train1)
x_test1 = sm.add_constant(X_test)

# Function to create the model
def build_ols_model(train):
  olsmodel = sm.OLS(y_train['Price_log'], train)
  return olsmodel.fit()

# Fit linear model on dataset
ols_model= build_ols_model(x_train1)
print(ols_model.summary())

# Retrieve coefficient and p-values into dataframe
olsmod = pd.DataFrame(ols_model.params, columns=['coef'])
olsmod['pval'] = ols_model.pvalues

# Filtering significant p-values(0.5)
olsmod = olsmod.sort_values(by = 'pval', ascending= True)
pval_filter = olsmod['pval']<= 0.05
olsmod[pval_filter]

# To separate improtant parmaters
imp_vars = olsmod[pval_filter].index.tolist()

# Getting overall variables
sig_var=[]
for col in imp_vars:
  if '' in col:
    first_part= col.split(' ')[0]
    for c in df_copy.columns:
      if first_part in c and c not in sig_var:
        sig_var.append(c)

start = '\033[1m'
end = '\033[95m'
print(start+ 'Most overall significant categorical varaibles of LINEAR REGRESSION  are ' +end,':\n', sig_var)

"""#### Build Ridge / Lasso Regression similar to Linear Regression:"""

# Create a Ridge regression model
ridge_model = Ridge(alpha=1.0)

# Fit Rifge regression model
ridge_model.fit(X_train, y_train['Price_log'])

#Get score of the model
ridge_score = evaluate_model(ridge_model)

"""The ridge regression results are very close to linear regression results, even slightly worst.

### **Decision Tree**
"""

# Create a decision tree regression model
dt_model = DecisionTreeRegressor(random_state = 1)

# Fit decision tree regression model
dt_model.fit(X_train, y_train['Price_log'])

# Get score of the model
dt_score = evaluate_model(dt_model)

"""Significantly better results than the previous linear modesl"""

# The importance of features in the tree building.
# The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.
# It is also known as the Gini importance.

print(pd.DataFrame(dt_model.feature_importances_, columns = ['Imp'], index = X_train.columns).sort_values(by = 'Imp', ascending = False))

"""* Power and Year are significantly most important feature based on the decision tree. This aligns with the finding from the statsmodel.
* In addition New price has a slight role too.

### **Random Forest**
"""

# Create a randomforest regression model
rf_model = RandomForestRegressor(random_state=1)

# Fit randomforest model
rf_model.fit(X_train, y_train['Price_log'])

# Get score of the model
rf_score = evaluate_model(rf_model)

"""* Results improved on the Decision Tree: The Rs value on the both training and test data sets improved.
* RMSE increase on the training data set, but decreased in the test set, showing that the overfitting is reduced.
* Besides the slight differences, both decision tree and random forest model appear suitable for this data, however there is still need for improvement. Because both model's results on training and test data show overfitting. Hyperparameter tuning will be done for the two methods.

### **Hyperparameter Tuning: Decision Tree**
"""

# Choose the type of estimator
dt_model_tuned = DecisionTreeRegressor(random_state = 1)

# Grid of parameters to choose from
# Check documentation for all the parametrs that the model takes and play with those
dt_parameters ={
    'max_depth': [2, 12, 20, None],  #The maximum levels of the tree. Deeper trees capture more patterns but overfit easily.
    'min_samples_split': [5, 15, 25, 35],#[2, 10, 20], #The minimum number of samples a node must have before it can be split.
    'criterion': ['squared_error', 'absolute_error'], #The function to measure the quality of a split (usually "gini" or "entropy").
    'min_samples_leaf': [1, 2, 4]
    }

# param_grid = {
#     'max_depth': [3, 5, 10, None],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4],
#     'criterion': ['gini', 'entropy']
# }
# Type of scoring used to compare parameter combinations
dt_scoring_metric = 'r2'

# Run the grid search
dt_grid_search = GridSearchCV(
    estimator = dt_model_tuned,
    param_grid = dt_parameters,
    scoring = dt_scoring_metric,
    cv = 5, #5-folds cross validation
    verbose = 1, # print updates
    n_jobs = -1 #use all CPU cores
)

# Set the model to the best combination of parameters
dt_grid_search.fit(X_train, y_train['Price_log'])

# Fit the best algorithm to the data
dt_model_tuned = dt_grid_search.best_estimator_

# Get score of the model
evaluate_model(dt_model_tuned)

"""**Feature Importance**"""

# Print important features of tuned decision tree
print(pd.DataFrame(dt_model_tuned.feature_importances_, columns = ['Imp'], index = X_train.columns).sort_values(by = 'Imp', ascending = False))

"""### **Hyperparameter Tuning: Random Forest**"""

# Choose the type of Regressor
rf_model_tuned =RandomForestRegressor(random_state=1)

# Define the parameters for Grid to choose from
rf_parameters = {
    'n_estimators': [100, 200, 300], # Number of trees
    'max_depth': [10, 20, None],     # Depth of trees ,
    # 'min_samples_leaf': [1, 4, 8],   # Minimum number of samples required to be at a leaf node
    'max_features': ['sqrt', 0.5]    # Number of features to consider for best split
}
# Type of scoring used to compare parameter combinations
#   - 'neg_mean_squared_error' (MSE)
#   - 'neg_mean_absolute_error' (MAE)
#   - 'r2'
rf_scoring_metric = 'neg_mean_squared_error' # Scikit-learn optimizes for the *highest* score, so negative is used for error metrics

# Run the grid search
rf_grid_search = GridSearchCV(
    estimator=rf_model_tuned,
    param_grid=rf_parameters,
    scoring=rf_scoring_metric,
    cv=5,                 # Use 5-fold cross-validation
    verbose=1,            # Print updates during the process
    n_jobs=-1             # Use all available CPU cores
)

# Set the model to the best combination of parameters
rf_grid_search.fit(X_train, y_train['Price_log'])

# Fit the best algorithm to the data
rf_model_tuned = rf_grid_search.best_estimator_

# Get score of the model
evaluate_model(rf_model_tuned)

# Choose the type of Regressor
rf_model_tuned1 =RandomForestRegressor(random_state=1)

# Define the parameters for Grid to choose from
rf_parameters1 = {
    'n_estimators': [400, 450], # Number of trees
    'max_depth': [10, 20],     # Depth of trees
    'min_samples_leaf': [1, 4, 8],   # Minimum number of samples required to be at a leaf node
    'max_features': [0.5]    # Number of features to consider for best split
}
# Type of scoring used to compare parameter combinations
#   - 'neg_mean_squared_error' (MSE)
#   - 'neg_mean_absolute_error' (MAE)
#   - 'r2'
rf_scoring_metric1 = 'neg_mean_squared_error' # Scikit-learn optimizes for the *highest* score, so negative is used for error metrics

# Run the grid search
rf_grid_search1 = GridSearchCV(
    estimator=rf_model_tuned1,
    param_grid=rf_parameters1,
    scoring=rf_scoring_metric1,
    cv=5,                 # Use 5-fold cross-validation
    verbose=1,            # Print updates during the process
    n_jobs=-1             # Use all available CPU cores
)

# Set the model to the best combination of parameters
rf_grid_search1.fit(X_train, y_train['Price_log'])

# Fit the best algorithm to the data
rf_model_tuned1 = rf_grid_search1.best_estimator_

# Get score of the model
evaluate_model(rf_model_tuned1)

"""**Feature Importance**"""

# Print important features of tuned decision tree
print(pd.DataFrame(rf_model_tuned.feature_importances_, columns = ['Imp'], index = X_train.columns).sort_values(by = 'Imp', ascending = False))

"""## **Conclusions and Recommendations**

Best model has the overall high R2 value and also most similar for training and datasets, with the least overfitting, which is the Linear Regression model:
* The model performs exceptionally well, explaining approximately 94.75% of the variance in the training data and 89.57% in the test data. with 95% and 90% for train and test data sets.
* The RMSE on the test set is 3.72 (Lakhs). While there is a slight gap between training and test performance (suggesting mild overfitting), the model is robust enough for business decision-making.

Key Drivers of Used Car Prices
Based on the OLS coefficients and EDA:

* Power & Engine: These are the strongest positive predictors. A higher BHP and Engine CC significantly increase the resale value.

* Year (Car Age): Newer models command a significant premium. The value of the car depreciates most sharply after the first 4â€“5 years.

* Transmission: Automatic cars have a significantly higher market value compared to Manual variants across almost all segments.

* Fuel Type: Diesel vehicles generally have higher resale values than Petrol, likely due to better fuel economy in the Indian market, though Electric cars show a high-price niche (despite limited data).

* Ownership: First-hand cars are priced much higher. Every additional previous owner significantly reduces the predicted price.

**Further Recommendation**

Cars4U can use the model to identify "underpriced" cars in the market. If the model predicts a price significantly higher than the current listing price, it indicates a high-margin acquisition opportunity. Conversely, it prevents the company from overpaying for inventory that has high "hidden" depreciation (e.g., high mileage or multiple owners).

In addition, "Location" shows that cities like Coimbatore and Bangalore tend to have higher price coefficients compared to Kolkata or Jaipur, likely due to local demand, taxes, and economic factors. This information can be useful, in case Cars4U has the capability to trasnfer cars to locations.
"""